{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  steps: 19\n",
      "Episode: 1  steps: 12\n",
      "Episode: 2  steps: 23\n",
      "Episode: 3  steps: 10\n",
      "Episode: 4  steps: 39\n",
      "Episode: 5  steps: 11\n",
      "Episode: 6  steps: 12\n",
      "Episode: 7  steps: 11\n",
      "Episode: 8  steps: 17\n",
      "Episode: 9  steps: 9\n",
      "Episode: 10  steps: 31\n",
      "Episode: 11  steps: 10\n",
      "Episode: 12  steps: 12\n",
      "Episode: 13  steps: 9\n",
      "Episode: 14  steps: 9\n",
      "Episode: 15  steps: 9\n",
      "Episode: 16  steps: 10\n",
      "Episode: 17  steps: 14\n",
      "Episode: 18  steps: 11\n",
      "Episode: 19  steps: 13\n",
      "Episode: 20  steps: 11\n",
      "Episode: 21  steps: 17\n",
      "Episode: 22  steps: 14\n",
      "Episode: 23  steps: 14\n",
      "Episode: 24  steps: 9\n",
      "Episode: 25  steps: 10\n",
      "Episode: 26  steps: 9\n",
      "Episode: 27  steps: 9\n",
      "Episode: 28  steps: 10\n",
      "Episode: 29  steps: 12\n",
      "Episode: 30  steps: 13\n",
      "Episode: 31  steps: 10\n",
      "Episode: 32  steps: 10\n",
      "Episode: 33  steps: 9\n",
      "Episode: 34  steps: 9\n",
      "Episode: 35  steps: 9\n",
      "Episode: 36  steps: 11\n",
      "Episode: 37  steps: 10\n",
      "Episode: 38  steps: 9\n",
      "Episode: 39  steps: 10\n",
      "Episode: 40  steps: 10\n",
      "Episode: 41  steps: 9\n",
      "Episode: 42  steps: 12\n",
      "Episode: 43  steps: 10\n",
      "Episode: 44  steps: 11\n",
      "Episode: 45  steps: 11\n",
      "Episode: 46  steps: 11\n",
      "Episode: 47  steps: 12\n",
      "Episode: 48  steps: 14\n",
      "Episode: 49  steps: 11\n",
      "Episode: 50  steps: 11\n",
      "Episode: 51  steps: 11\n",
      "Episode: 52  steps: 11\n",
      "Episode: 53  steps: 11\n",
      "Episode: 54  steps: 9\n",
      "Episode: 55  steps: 12\n",
      "Episode: 56  steps: 10\n",
      "Episode: 57  steps: 11\n",
      "Episode: 58  steps: 12\n",
      "Episode: 59  steps: 13\n",
      "Episode: 60  steps: 13\n",
      "Episode: 61  steps: 12\n",
      "Episode: 62  steps: 10\n",
      "Episode: 63  steps: 15\n",
      "Episode: 64  steps: 12\n",
      "Episode: 65  steps: 11\n",
      "Episode: 66  steps: 10\n",
      "Episode: 67  steps: 11\n",
      "Episode: 68  steps: 10\n",
      "Episode: 69  steps: 12\n",
      "Episode: 70  steps: 13\n",
      "Episode: 71  steps: 13\n",
      "Episode: 72  steps: 11\n",
      "Episode: 73  steps: 13\n",
      "Episode: 74  steps: 18\n",
      "Episode: 75  steps: 14\n",
      "Episode: 76  steps: 16\n",
      "Episode: 77  steps: 20\n",
      "Episode: 78  steps: 16\n",
      "Episode: 79  steps: 14\n",
      "Episode: 80  steps: 12\n",
      "Episode: 81  steps: 14\n",
      "Episode: 82  steps: 15\n",
      "Episode: 83  steps: 21\n",
      "Episode: 84  steps: 18\n",
      "Episode: 85  steps: 14\n",
      "Episode: 86  steps: 17\n",
      "Episode: 87  steps: 22\n",
      "Episode: 88  steps: 21\n",
      "Episode: 89  steps: 19\n",
      "Episode: 90  steps: 26\n",
      "Episode: 91  steps: 40\n",
      "Episode: 92  steps: 25\n",
      "Episode: 93  steps: 23\n",
      "Episode: 94  steps: 20\n",
      "Episode: 95  steps: 61\n",
      "Episode: 96  steps: 34\n",
      "Episode: 97  steps: 10\n",
      "Episode: 98  steps: 13\n",
      "Episode: 99  steps: 12\n",
      "Episode: 100  steps: 9\n",
      "Episode: 101  steps: 10\n",
      "Episode: 102  steps: 9\n",
      "Episode: 103  steps: 9\n",
      "Episode: 104  steps: 12\n",
      "Episode: 105  steps: 14\n",
      "Episode: 106  steps: 43\n",
      "Episode: 107  steps: 34\n",
      "Episode: 108  steps: 44\n",
      "Episode: 109  steps: 40\n",
      "Episode: 110  steps: 58\n",
      "Episode: 111  steps: 31\n",
      "Episode: 112  steps: 31\n",
      "Episode: 113  steps: 42\n",
      "Episode: 114  steps: 39\n",
      "Episode: 115  steps: 37\n",
      "Episode: 116  steps: 31\n",
      "Episode: 117  steps: 44\n",
      "Episode: 118  steps: 38\n",
      "Episode: 119  steps: 30\n",
      "Episode: 120  steps: 24\n",
      "Episode: 121  steps: 35\n",
      "Episode: 122  steps: 46\n",
      "Episode: 123  steps: 29\n",
      "Episode: 124  steps: 30\n",
      "Episode: 125  steps: 40\n",
      "Episode: 126  steps: 35\n",
      "Episode: 127  steps: 53\n",
      "Episode: 128  steps: 43\n",
      "Episode: 129  steps: 36\n",
      "Episode: 130  steps: 41\n",
      "Episode: 131  steps: 41\n",
      "Episode: 132  steps: 51\n",
      "Episode: 133  steps: 55\n",
      "Episode: 134  steps: 48\n",
      "Episode: 135  steps: 58\n",
      "Episode: 136  steps: 34\n",
      "Episode: 137  steps: 120\n",
      "Episode: 138  steps: 69\n",
      "Episode: 139  steps: 91\n",
      "Episode: 140  steps: 73\n",
      "Episode: 141  steps: 109\n",
      "Episode: 142  steps: 86\n",
      "Episode: 143  steps: 47\n",
      "Episode: 144  steps: 44\n",
      "Episode: 145  steps: 76\n",
      "Episode: 146  steps: 94\n",
      "Episode: 147  steps: 43\n",
      "Episode: 148  steps: 41\n",
      "Episode: 149  steps: 138\n",
      "Episode: 150  steps: 41\n",
      "Episode: 151  steps: 65\n",
      "Episode: 152  steps: 53\n",
      "Episode: 153  steps: 58\n",
      "Episode: 154  steps: 86\n",
      "Episode: 155  steps: 47\n",
      "Episode: 156  steps: 86\n",
      "Episode: 157  steps: 97\n",
      "Episode: 158  steps: 77\n",
      "Episode: 159  steps: 97\n",
      "Episode: 160  steps: 232\n",
      "Episode: 161  steps: 88\n",
      "Episode: 162  steps: 66\n",
      "Episode: 163  steps: 149\n",
      "Episode: 164  steps: 97\n",
      "Episode: 165  steps: 77\n",
      "Episode: 166  steps: 92\n",
      "Episode: 167  steps: 94\n",
      "Episode: 168  steps: 117\n",
      "Episode: 169  steps: 87\n",
      "Episode: 170  steps: 130\n",
      "Episode: 171  steps: 135\n",
      "Episode: 172  steps: 119\n",
      "Episode: 173  steps: 143\n",
      "Episode: 174  steps: 131\n",
      "Episode: 175  steps: 125\n",
      "Episode: 176  steps: 206\n",
      "Episode: 177  steps: 150\n",
      "Episode: 178  steps: 107\n",
      "Episode: 179  steps: 86\n",
      "Episode: 180  steps: 145\n",
      "Episode: 181  steps: 142\n",
      "Episode: 182  steps: 196\n",
      "Episode: 183  steps: 170\n",
      "Episode: 184  steps: 379\n",
      "Episode: 185  steps: 184\n",
      "Episode: 186  steps: 138\n",
      "Episode: 187  steps: 243\n",
      "Episode: 188  steps: 154\n",
      "Episode: 189  steps: 139\n",
      "Episode: 190  steps: 133\n",
      "Episode: 191  steps: 164\n",
      "Episode: 192  steps: 162\n",
      "Episode: 193  steps: 202\n",
      "Episode: 194  steps: 192\n",
      "Episode: 195  steps: 232\n",
      "Episode: 196  steps: 245\n",
      "Episode: 197  steps: 276\n",
      "Episode: 198  steps: 231\n",
      "Episode: 199  steps: 215\n",
      "Episode: 200  steps: 277\n",
      "Episode: 201  steps: 282\n",
      "Episode: 202  steps: 249\n",
      "Episode: 203  steps: 290\n",
      "Episode: 204  steps: 293\n",
      "Episode: 205  steps: 211\n",
      "Episode: 206  steps: 221\n",
      "Episode: 207  steps: 213\n",
      "Episode: 208  steps: 238\n",
      "Episode: 209  steps: 195\n",
      "Episode: 210  steps: 235\n",
      "Episode: 211  steps: 194\n",
      "Episode: 212  steps: 197\n",
      "Episode: 213  steps: 291\n",
      "Episode: 214  steps: 218\n",
      "Episode: 215  steps: 187\n",
      "Episode: 216  steps: 172\n",
      "Episode: 217  steps: 211\n",
      "Episode: 218  steps: 224\n",
      "Episode: 219  steps: 151\n",
      "Episode: 220  steps: 189\n",
      "Episode: 221  steps: 186\n",
      "Episode: 222  steps: 169\n",
      "Episode: 223  steps: 230\n",
      "Episode: 224  steps: 196\n",
      "Episode: 225  steps: 227\n",
      "Episode: 226  steps: 198\n",
      "Episode: 227  steps: 182\n",
      "Episode: 228  steps: 187\n",
      "Episode: 229  steps: 217\n",
      "Episode: 230  steps: 203\n",
      "Episode: 231  steps: 176\n",
      "Episode: 232  steps: 180\n",
      "Episode: 233  steps: 171\n",
      "Episode: 234  steps: 187\n",
      "Episode: 235  steps: 190\n",
      "Episode: 236  steps: 174\n",
      "Episode: 237  steps: 149\n",
      "Episode: 238  steps: 161\n",
      "Episode: 239  steps: 189\n",
      "Episode: 240  steps: 199\n",
      "Episode: 241  steps: 178\n",
      "Episode: 242  steps: 184\n",
      "Episode: 243  steps: 181\n",
      "Episode: 244  steps: 183\n",
      "Episode: 245  steps: 173\n",
      "Episode: 246  steps: 182\n",
      "Episode: 247  steps: 205\n",
      "Episode: 248  steps: 236\n",
      "Episode: 249  steps: 165\n",
      "Episode: 250  steps: 228\n",
      "Episode: 251  steps: 180\n",
      "Episode: 252  steps: 309\n",
      "Episode: 253  steps: 171\n",
      "Episode: 254  steps: 201\n",
      "Episode: 255  steps: 229\n",
      "Episode: 256  steps: 233\n",
      "Episode: 257  steps: 215\n",
      "Episode: 258  steps: 270\n",
      "Episode: 259  steps: 204\n",
      "Episode: 260  steps: 150\n",
      "Episode: 261  steps: 196\n",
      "Episode: 262  steps: 194\n",
      "Episode: 263  steps: 180\n",
      "Episode: 264  steps: 210\n",
      "Episode: 265  steps: 196\n",
      "Episode: 266  steps: 198\n",
      "Episode: 267  steps: 195\n",
      "Episode: 268  steps: 220\n",
      "Episode: 269  steps: 221\n",
      "Episode: 270  steps: 184\n",
      "Episode: 271  steps: 230\n",
      "Episode: 272  steps: 172\n",
      "Episode: 273  steps: 185\n",
      "Game Cleared in 273 episodes with avg reward 199.14\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Double DQN (Nature 2015)\n",
    "http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
    "\n",
    "Notes:\n",
    "    The difference is that now there are two DQNs (DQN & Target DQN)\n",
    "\n",
    "    y_i = r_i + ùõæ * max(Q(next_state, action; ùúÉ_target))\n",
    "\n",
    "    Loss: (y_i - Q(state, action; ùúÉ))^2\n",
    "\n",
    "    Every C step, ùúÉ_target <- ùúÉ\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import dqn\n",
    "\n",
    "import gym\n",
    "from typing import List\n",
    "\n",
    "import time\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 10001\n",
    "# env = gym.wrappers.Monitor(env, directory=\"gym-results/\", force=True)\n",
    "\n",
    "# Constants defining our neural network\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "REPLAY_MEMORY = 50000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQUENCY = 5\n",
    "MAX_EPISODES = 5000\n",
    "\n",
    "\n",
    "def replay_train(mainDQN: dqn.DQN, targetDQN: dqn.DQN, train_batch: list) -> float:\n",
    "    \"\"\"Trains `mainDQN` with target Q values given by `targetDQN`\n",
    "\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): Main DQN that will be trained\n",
    "        targetDQN (dqn.DQN): Target DQN that will predict Q_target\n",
    "        train_batch (list): Minibatch of replay memory\n",
    "            Each element is (s, a, r, s', done)\n",
    "            [(state, action, reward, next_state, done), ...]\n",
    "\n",
    "    Returns:\n",
    "        float: After updating `mainDQN`, it returns a `loss`\n",
    "    \"\"\"\n",
    "    states = np.vstack([x[0] for x in train_batch])\n",
    "    actions = np.array([x[1] for x in train_batch])\n",
    "    rewards = np.array([x[2] for x in train_batch])\n",
    "    next_states = np.vstack([x[3] for x in train_batch])\n",
    "    done = np.array([x[4] for x in train_batch])\n",
    "\n",
    "    X = states\n",
    "\n",
    "    Q_target = rewards + DISCOUNT_RATE * np.max(targetDQN.predict(next_states), axis=1) * ~done\n",
    "\n",
    "    y = mainDQN.predict(states)\n",
    "    y[np.arange(len(X)), actions] = Q_target\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    return mainDQN.update(X, y)\n",
    "\n",
    "\n",
    "def get_copy_var_ops(*, dest_scope_name: str, src_scope_name: str) -> List[tf.Operation]:\n",
    "    \"\"\"Creates TF operations that copy weights from `src_scope` to `dest_scope`\n",
    "\n",
    "    Args:\n",
    "        dest_scope_name (str): Destination weights (copy to)\n",
    "        src_scope_name (str): Source weight (copy from)\n",
    "\n",
    "    Returns:\n",
    "        List[tf.Operation]: Update operations are created and returned\n",
    "    \"\"\"\n",
    "    # Copy variables src_scope to dest_scope\n",
    "    op_holder = []\n",
    "\n",
    "    src_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "    dest_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
    "\n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def bot_play(mainDQN: dqn.DQN, env: gym.Env) -> None:\n",
    "    \"\"\"Test runs with rendering and prints the total score\n",
    "\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): DQN agent to run a test\n",
    "        env (gym.Env): Gym Environment\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # env.render()\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(reward_sum))\n",
    "            break\n",
    "\n",
    "\n",
    "def main():\n",
    "    timestamp = time.time()\n",
    "    \n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
    "\n",
    "    last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=f\"main_{timestamp}\")\n",
    "        targetDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=f\"target_{timestamp}\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # initial copy q_net -> target_net\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name=f\"target_{timestamp}\",\n",
    "                                    src_scope_name=f\"main_{timestamp}\")\n",
    "        sess.run(copy_ops)\n",
    "\n",
    "        for episode in range(MAX_EPISODES):\n",
    "            e = 1. / ((episode / 10) + 1)\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            state = env.reset()\n",
    "\n",
    "            while not done:\n",
    "                if np.random.rand() < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    # Choose an action by greedily from the Q-network\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "\n",
    "                # Get new state and reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if done:  # Penalty\n",
    "                    reward = -1\n",
    "\n",
    "                # Save the experience to our buffer\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                if len(replay_buffer) > BATCH_SIZE:\n",
    "                    minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                    loss, _ = replay_train(mainDQN, targetDQN, minibatch)\n",
    "\n",
    "                if step_count % TARGET_UPDATE_FREQUENCY == 0:\n",
    "                    sess.run(copy_ops)\n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "\n",
    "            print(\"Episode: {}  steps: {}\".format(episode, step_count))\n",
    "\n",
    "            # CartPole-v0 Game Clear Checking Logic\n",
    "            last_100_game_reward.append(step_count)\n",
    "\n",
    "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "                avg_reward = np.mean(last_100_game_reward)\n",
    "\n",
    "                if avg_reward > 199:\n",
    "                    print(f\"Game Cleared in {episode} episodes with avg reward {avg_reward}\")\n",
    "                    break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
