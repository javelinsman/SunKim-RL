{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAACi0lEQVR4nO2WTUhUURiGnzsYtAxJWoStShiwIEghhMHaGLmIgoGgNkUUREGbFoHLkBYtpIIcKCISFxXipkBzUVKLyo0iw0hYTiX9mWERM6PjfC3unbnnzpx7zvysAs/q/Hzvc997znd+HKGxEmlQvwHYAPwfgFuO075ujHCMmym3GdgzXb+DGMCMMcQIWHoDMGIEIGEl+yRuiRARkbDhWf8T03UBDqouP9UBEJEVgG0ANH+vBxCF7SJyF4C9uZoBi6X5cxHDtQKAiWK9D6C9NsBYYAEzWwE+1AIAfqjtUYCB6gG9cCLY8xegOV8l4L0uA88BzFQHAKZERIZRM2AC4Go1gH7oFhEZgLjavwzQUrAClos/ULmXOgHmbIDiD0gfMBIcSwBcMQP6oderdpevpsg8gD81GsBn1TdAtiwgBtCTDwUAyVJjFWgpjxgH4FRBDzgEp5XmPl1GZN1z4owO8DQoGAo51K67iGQF4BfwxW9eAkhoALIAwOUKADDoO40AXBMpPDweiz9YD4Y+64BUOeAAtJYaowA7/8jsJu9sfKFxEgTcA4qJutYKkJbb5jsg0DcHzHv1MYBHvw+rp7MNkFMmYD9wNOXpdrzeBcBjCwDodGtLAMc8+cWCm5BavQroKXn8qbi+6bHbpnRyFXDDz/rzJfm4XlUBuJ+USfwr7KOr7vpmlXuAFRKLwCu/OzP5ctWrPh+yA3aTBu6ERJC2AdYAuBAWEdicWsBZgI7QCLaYAI6AAzgFQooDpodYBPIAoXrzIw8i0ARHwr+RtQCaMDskY3dgLl8bBSw0CnhrGTe/lYFoyrqM5pKCqGncBngHdDUCaAPipgDLHKSiWPLE4iAKnDRGWBw4NgP8A69M+9lDNVPpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64 at 0x1CD96902EF0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image, ImageFont, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "# image = Image.new('L', (64, 64))\n",
    "image = Image.fromarray((np.zeros((64, 64)) + 255).astype('uint8'))\n",
    "draw = ImageDraw.Draw(image)\n",
    "font_block = ImageFont.truetype('Deng.ttf', 64)\n",
    "font_cursive = ImageFont.truetype('xukeli.ttf', 64)\n",
    "draw.text((0, 0), 'ä»Š', font=font_cursive)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import dqn\n",
    "\n",
    "import gym\n",
    "from typing import List\n",
    "\n",
    "import time\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 10001\n",
    "# env = gym.wrappers.Monitor(env, directory=\"gym-results/\", force=True)\n",
    "\n",
    "# Constants defining our neural network\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "REPLAY_MEMORY = 50000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQUENCY = 5\n",
    "MAX_EPISODES = 5000\n",
    "\n",
    "\n",
    "def replay_train(mainDQN: dqn.DQN, targetDQN: dqn.DQN, train_batch: list) -> float:\n",
    "    \"\"\"Trains `mainDQN` with target Q values given by `targetDQN`\n",
    "\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): Main DQN that will be trained\n",
    "        targetDQN (dqn.DQN): Target DQN that will predict Q_target\n",
    "        train_batch (list): Minibatch of replay memory\n",
    "            Each element is (s, a, r, s', done)\n",
    "            [(state, action, reward, next_state, done), ...]\n",
    "\n",
    "    Returns:\n",
    "        float: After updating `mainDQN`, it returns a `loss`\n",
    "    \"\"\"\n",
    "    states = np.vstack([x[0] for x in train_batch])\n",
    "    actions = np.array([x[1] for x in train_batch])\n",
    "    rewards = np.array([x[2] for x in train_batch])\n",
    "    next_states = np.vstack([x[3] for x in train_batch])\n",
    "    done = np.array([x[4] for x in train_batch])\n",
    "\n",
    "    X = states\n",
    "\n",
    "    Q_target = rewards + DISCOUNT_RATE * np.max(targetDQN.predict(next_states), axis=1) * ~done\n",
    "\n",
    "    y = mainDQN.predict(states)\n",
    "    y[np.arange(len(X)), actions] = Q_target\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    return mainDQN.update(X, y)\n",
    "\n",
    "\n",
    "def get_copy_var_ops(*, dest_scope_name: str, src_scope_name: str) -> List[tf.Operation]:\n",
    "    \"\"\"Creates TF operations that copy weights from `src_scope` to `dest_scope`\n",
    "\n",
    "    Args:\n",
    "        dest_scope_name (str): Destination weights (copy to)\n",
    "        src_scope_name (str): Source weight (copy from)\n",
    "\n",
    "    Returns:\n",
    "        List[tf.Operation]: Update operations are created and returned\n",
    "    \"\"\"\n",
    "    # Copy variables src_scope to dest_scope\n",
    "    op_holder = []\n",
    "\n",
    "    src_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "    dest_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
    "\n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def bot_play(mainDQN: dqn.DQN, env: gym.Env) -> None:\n",
    "    \"\"\"Test runs with rendering and prints the total score\n",
    "\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): DQN agent to run a test\n",
    "        env (gym.Env): Gym Environment\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # env.render()\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(reward_sum))\n",
    "            break\n",
    "\n",
    "\n",
    "def main():\n",
    "    timestamp = time.time()\n",
    "    \n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
    "\n",
    "    last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=f\"main_{timestamp}\")\n",
    "        targetDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=f\"target_{timestamp}\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # initial copy q_net -> target_net\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name=f\"target_{timestamp}\",\n",
    "                                    src_scope_name=f\"main_{timestamp}\")\n",
    "        sess.run(copy_ops)\n",
    "\n",
    "        for episode in range(MAX_EPISODES):\n",
    "            e = 1. / ((episode / 10) + 1)\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            state = env.reset()\n",
    "\n",
    "            while not done:\n",
    "                if np.random.rand() < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    # Choose an action by greedily from the Q-network\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "\n",
    "                # Get new state and reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if done:  # Penalty\n",
    "                    reward = -1\n",
    "\n",
    "                # Save the experience to our buffer\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                if len(replay_buffer) > BATCH_SIZE:\n",
    "                    minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                    loss, _ = replay_train(mainDQN, targetDQN, minibatch)\n",
    "\n",
    "                if step_count % TARGET_UPDATE_FREQUENCY == 0:\n",
    "                    sess.run(copy_ops)\n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "\n",
    "            print(\"Episode: {}  steps: {}\".format(episode, step_count))\n",
    "\n",
    "            # CartPole-v0 Game Clear Checking Logic\n",
    "            last_100_game_reward.append(step_count)\n",
    "\n",
    "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "                avg_reward = np.mean(last_100_game_reward)\n",
    "\n",
    "                if avg_reward > 199:\n",
    "                    print(f\"Game Cleared in {episode} episodes with avg reward {avg_reward}\")\n",
    "                    break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-d6579f534729>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
